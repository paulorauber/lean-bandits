/-
Copyright (c) 2025 RÃ©my Degenne. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: RÃ©my Degenne, Paulo Rauber
-/
import LeanBandits.ForMathlib.Measurable
import LeanBandits.ForMathlib.Traj
import LeanBandits.ForMathlib.CondDistrib
import Mathlib.Probability.HasLaw

/-!
# Algorithms
-/

open MeasureTheory ProbabilityTheory Filter Real Finset

open scoped ENNReal NNReal

namespace Learning

variable {Î± R : Type*} {mÎ± : MeasurableSpace Î±} {mR : MeasurableSpace R}

/-- A stochastic, sequential algorithm. -/
structure Algorithm (Î± R : Type*) [MeasurableSpace Î±] [MeasurableSpace R] where
  /-- Policy or sampling rule: distribution of the next action. -/
  policy : (n : â„•) â†’ Kernel (Iic n â†’ Î± Ã— R) Î±
  [h_policy : âˆ€ n, IsMarkovKernel (policy n)]
  /-- Distribution of the first action. -/
  p0 : Measure Î±
  [hp0 : IsProbabilityMeasure p0]

instance (alg : Algorithm Î± R) (n : â„•) : IsMarkovKernel (alg.policy n) := alg.h_policy n
instance (alg : Algorithm Î± R) : IsProbabilityMeasure alg.p0 := alg.hp0

/-- A stochastic environment. -/
structure Environment (Î± R : Type*) [MeasurableSpace Î±] [MeasurableSpace R] where
  /-- Distribution of the next observation as function of the past history. -/
  feedback : (n : â„•) â†’ Kernel ((Iic n â†’ Î± Ã— R) Ã— Î±) R
  [h_feedback : âˆ€ n, IsMarkovKernel (feedback n)]
  /-- Distribution of the first observation given the first action. -/
  Î½0 : Kernel Î± R
  [hp0 : IsMarkovKernel Î½0]

instance (env : Environment Î± R) (n : â„•) : IsMarkovKernel (env.feedback n) := env.h_feedback n
instance (env : Environment Î± R) : IsMarkovKernel env.Î½0 := env.hp0

/-- Kernel describing the distribution of the next action-reward pair given the history
up to `n`. -/
noncomputable
def stepKernel (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    Kernel (Iic n â†’ Î± Ã— R) (Î± Ã— R) :=
  alg.policy n âŠ—â‚– env.feedback n
deriving IsMarkovKernel

@[simp]
lemma fst_stepKernel (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    (stepKernel alg env n).fst = alg.policy n := by
  rw [stepKernel, Kernel.fst_compProd]

/-- Kernel sending a partial trajectory of the bandit interaction `Iic n â†’ Î± Ã— â„` to a measure
on `â„• â†’ Î± Ã— â„`, supported on full trajectories that start with the partial one. -/
noncomputable def traj (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    Kernel (Iic n â†’ Î± Ã— R) (â„• â†’ Î± Ã— R) :=
  ProbabilityTheory.Kernel.traj (X := fun _ â†¦ Î± Ã— R) (stepKernel alg env) n
deriving IsMarkovKernel

/-- Measure on the sequence of actions and observations generated by the algorithm/environment. -/
noncomputable
def trajMeasure (alg : Algorithm Î± R) (env : Environment Î± R) :
    Measure (â„• â†’ Î± Ã— R) :=
  Kernel.trajMeasure (alg.p0 âŠ—â‚˜ env.Î½0) (stepKernel alg env)
deriving IsProbabilityMeasure

/-- Action and reward at step `n`. -/
def step (n : â„•) (h : â„• â†’ Î± Ã— R) : Î± Ã— R := h n

/-- `action n` is the action pulled at time `n`. This is a random variable on the measurable space
`â„• â†’ Î± Ã— â„`. -/
def action (n : â„•) (h : â„• â†’ Î± Ã— R) : Î± := (h n).1

/-- `reward n` is the reward at time `n`. This is a random variable on the measurable space
`â„• â†’ Î± Ã— R`. -/
def reward (n : â„•) (h : â„• â†’ Î± Ã— R) : R := (h n).2

/-- `hist n` is the history up to time `n`. This is a random variable on the measurable space
`â„• â†’ Î± Ã— R`. -/
def hist (n : â„•) (h : â„• â†’ Î± Ã— R) : Iic n â†’ Î± Ã— R := fun i â†¦ h i

lemma fst_comp_step (n : â„•) : Prod.fst âˆ˜ step (Î± := Î±) (R := R) n = action n := rfl

@[fun_prop]
lemma measurable_step (n : â„•) : Measurable (step n (Î± := Î±) (R := R)) := by
  unfold step; fun_prop

@[fun_prop]
lemma measurable_step_prod : Measurable (fun p : â„• Ã— (â„• â†’ Î± Ã— R) â†¦ step p.1 p.2) := by
  refine measurable_from_prod_countable_right fun n â†¦ ?_
  simp only
  fun_prop

@[fun_prop]
lemma measurable_action (n : â„•) : Measurable (action n (Î± := Î±) (R := R)) := by
  unfold action; fun_prop

@[fun_prop]
lemma measurable_action_prod : Measurable (fun p : â„• Ã— (â„• â†’ Î± Ã— R) â†¦ action p.1 p.2) := by
  refine measurable_from_prod_countable_right fun n â†¦ ?_
  simp only
  fun_prop

@[fun_prop]
lemma measurable_reward (n : â„•) : Measurable (reward n (Î± := Î±) (R := R)) := by
  unfold reward; fun_prop

@[fun_prop]
lemma measurable_reward_prod : Measurable (fun p : â„• Ã— (â„• â†’ Î± Ã— R) â†¦ reward p.1 p.2) := by
  refine measurable_from_prod_countable_right fun n â†¦ ?_
  simp only
  fun_prop

@[fun_prop]
lemma measurable_hist (n : â„•) : Measurable (hist n (Î± := Î±) (R := R)) := by unfold hist; fun_prop

lemma hist_eq_frestrictLe :
    hist = Preorder.frestrictLe (Â«Ï€Â» := fun _ â†¦ Î± Ã— R) := by
  ext n h i : 3
  simp [hist, Preorder.frestrictLe]

/-- Filtration of the algorithm interaction. -/
protected def filtration (Î± R : Type*) [MeasurableSpace Î±] [MeasurableSpace R] :
    Filtration â„• (inferInstance : MeasurableSpace (â„• â†’ Î± Ã— R)) :=
  MeasureTheory.Filtration.piLE (X := fun _ â†¦ Î± Ã— R)

lemma step_eq_eval_comp_hist (n : â„•) :
    step (Î± := Î±) (R := R) n = (fun x â†¦ x âŸ¨n, by simpâŸ©) âˆ˜ (hist n) := rfl

lemma action_eq_eval_comp_hist (n : â„•) :
    action (Î± := Î±) (R := R) n = (fun x â†¦ (x âŸ¨n, by simpâŸ©).1) âˆ˜ (hist n) := rfl

lemma reward_eq_eval_comp_hist (n : â„•) :
    reward (Î± := Î±) (R := R) n = (fun x â†¦ (x âŸ¨n, by simpâŸ©).2) âˆ˜ (hist n) := rfl

lemma measurable_step_filtration (n : â„•) : Measurable[Learning.filtration Î± R n] (step n) := by
  simp only [Learning.filtration, Filtration.piLE_eq_comap_frestrictLe, â† hist_eq_frestrictLe]
  rw [step_eq_eval_comp_hist]
  exact measurable_comp_comap _ (by fun_prop)

lemma adapted_step [TopologicalSpace Î±] [TopologicalSpace.PseudoMetrizableSpace Î±]
    [SecondCountableTopology Î±] [OpensMeasurableSpace Î±]
    [TopologicalSpace R] [TopologicalSpace.PseudoMetrizableSpace R]
    [SecondCountableTopology R] [OpensMeasurableSpace R] :
    Adapted (Learning.filtration Î± R) (fun n â†¦ step (Î± := Î±) (R := R) n) :=
  fun n â†¦ (measurable_step_filtration n).stronglyMeasurable

lemma measurable_hist_filtration (n : â„•) : Measurable[Learning.filtration Î± R n] (hist n) := by
  simp [Learning.filtration, Filtration.piLE_eq_comap_frestrictLe, â† hist_eq_frestrictLe,
    measurable_iff_comap_le]

lemma adapted_hist [TopologicalSpace Î±] [TopologicalSpace.PseudoMetrizableSpace Î±]
    [SecondCountableTopology Î±] [OpensMeasurableSpace Î±]
    [TopologicalSpace R] [TopologicalSpace.PseudoMetrizableSpace R]
    [SecondCountableTopology R] [OpensMeasurableSpace R] :
    Adapted (Learning.filtration Î± R) hist :=
  fun n â†¦ (measurable_hist_filtration n).stronglyMeasurable

lemma measurable_action_filtration (n : â„•) : Measurable[Learning.filtration Î± R n] (action n) := by
  simp only [Learning.filtration, Filtration.piLE_eq_comap_frestrictLe, â† hist_eq_frestrictLe]
  rw [action_eq_eval_comp_hist]
  exact measurable_comp_comap _ (by fun_prop)

lemma adapted_action [TopologicalSpace Î±] [TopologicalSpace.PseudoMetrizableSpace Î±]
    [SecondCountableTopology Î±] [OpensMeasurableSpace Î±] :
    Adapted (Learning.filtration Î± R) action :=
  fun n â†¦ (measurable_action_filtration n).stronglyMeasurable

lemma measurable_reward_filtration (n : â„•) : Measurable[Learning.filtration Î± R n] (reward n) := by
  simp only [Learning.filtration, Filtration.piLE_eq_comap_frestrictLe, â† hist_eq_frestrictLe]
  rw [reward_eq_eval_comp_hist]
  exact measurable_comp_comap _ (by fun_prop)

lemma adapted_reward [TopologicalSpace R] [TopologicalSpace.PseudoMetrizableSpace R]
    [SecondCountableTopology R] [OpensMeasurableSpace R] :
    Adapted (Learning.filtration Î± R) reward :=
  fun n â†¦ (measurable_reward_filtration n).stronglyMeasurable

lemma condDistrib_step [StandardBorelSpace Î±] [Nonempty Î±] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    condDistrib (step (n + 1)) (hist n) (trajMeasure alg env)
      =áµ[(trajMeasure alg env).map (hist n)] stepKernel alg env n :=
  Kernel.condDistrib_trajMeasure

lemma condDistrib_action [StandardBorelSpace Î±] [Nonempty Î±] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    condDistrib (action (n + 1)) (hist n) (trajMeasure alg env)
      =áµ[(trajMeasure alg env).map (hist n)] alg.policy n := by
  rw [â† fst_comp_step]
  refine (condDistrib_comp _ (by fun_prop) (by fun_prop)).trans ?_
  filter_upwards [condDistrib_step alg env n] with h h_eq
  rw [Kernel.map_apply _ (by fun_prop), h_eq, â† Kernel.map_apply _ (by fun_prop), â† Kernel.fst_eq,
    fst_stepKernel]

lemma condDistrib_reward [StandardBorelSpace Î±] [Nonempty Î±] [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm Î± R) (env : Environment Î± R) (n : â„•) :
    condDistrib (reward (n + 1)) (fun Ï‰ â†¦ (hist n Ï‰, action (n + 1) Ï‰)) (trajMeasure alg env)
      =áµ[(trajMeasure alg env).map (fun Ï‰ â†¦ (hist n Ï‰, action (n + 1) Ï‰))] env.feedback n := by
  have h_step := condDistrib_step alg env n
  have h_action := condDistrib_action alg env n
  rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)] at h_step h_action âŠ¢
  rw [h_action, â† Measure.compProd_assoc, â† stepKernel, â† h_step,
    Measure.map_map (by fun_prop) (by fun_prop)]
  rfl

lemma hasLaw_step_zero (alg : Algorithm Î± R) (env : Environment Î± R) :
    HasLaw (step 0) (alg.p0 âŠ—â‚˜ env.Î½0) (trajMeasure alg env) where
  aemeasurable := Measurable.aemeasurable (by fun_prop)
  map_eq := by
    unfold step
    simp only [trajMeasure, Kernel.trajMeasure]
    rw [â† Measure.deterministic_comp_eq_map (by fun_prop), Measure.comp_assoc,
      Kernel.deterministic_comp_eq_map, Kernel.traj_zero_map_eval_zero,
      Measure.deterministic_comp_eq_map, Measure.map_map (by fun_prop) (by fun_prop)]
    convert Measure.map_id using 1

lemma hasLaw_action_zero (alg : Algorithm Î± R) (env : Environment Î± R) :
    HasLaw (action 0) alg.p0 (trajMeasure alg env) where
  map_eq := by
    rw [â† fst_comp_step, â† Measure.map_map (by fun_prop) (by fun_prop),
      (hasLaw_step_zero alg env).map_eq, â† Measure.fst, Measure.fst_compProd]

lemma condDistrib_reward_zero [StandardBorelSpace R] [Nonempty R]
    (alg : Algorithm Î± R) (env : Environment Î± R) :
    condDistrib (reward 0) (action 0) (trajMeasure alg env)
      =áµ[(trajMeasure alg env).map (action 0)] env.Î½0 := by
  have h_step := (hasLaw_step_zero alg env).map_eq
  have h_action := (hasLaw_action_zero alg env).map_eq
  rwa [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop), h_action]

section stationaryEnv

/-- A stationary environment, in which the distribution of the next reward depends only on the last
action. -/
@[simps]
def stationaryEnv (Î½ : Kernel Î± R) [IsMarkovKernel Î½] : Environment Î± R where
  feedback _ := Î½.prodMkLeft _
  Î½0 := Î½

variable {alg : Algorithm Î± R} {Î½ : Kernel Î± R} [IsMarkovKernel Î½]

local notation "ğ”“" => trajMeasure alg (stationaryEnv Î½)

lemma condDistrib_reward_stationaryEnv [StandardBorelSpace Î±] [Nonempty Î±]
    [StandardBorelSpace R] [Nonempty R] (n : â„•) :
    condDistrib (reward n) (action n) ğ”“ =áµ[(ğ”“).map (action n)] Î½ := by
  cases n with
  | zero =>
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)]
    change (ğ”“).map (step 0) = (ğ”“).map (action 0) âŠ—â‚˜ Î½
    rw [(hasLaw_action_zero alg (stationaryEnv Î½)).map_eq,
      (hasLaw_step_zero alg (stationaryEnv Î½)).map_eq, stationaryEnv_Î½0]
  | succ n =>
    have h_eq := condDistrib_reward alg (stationaryEnv Î½) n
    rw [condDistrib_ae_eq_iff_measure_eq_compProd _ (by fun_prop)] at h_eq âŠ¢
    have : (ğ”“).map (action (n + 1)) = ((ğ”“).map (fun x â†¦ (hist n x, action (n + 1) x))).snd := by
      rw [Measure.snd_map_prodMk (by fun_prop)]
    simp only [stationaryEnv_feedback] at h_eq
    rw [this, â† Measure.snd_prodAssoc_compProd_prodMkLeft, â† h_eq,
      Measure.snd_map_prodMk (by fun_prop), Measure.map_map (by fun_prop) (by fun_prop)]
    congr

lemma condIndepFun_reward_hist_action [StandardBorelSpace Î±] [Nonempty Î±]
    [StandardBorelSpace R] [Nonempty R] (n : â„•) :
    CondIndepFun (MeasurableSpace.comap (action (n + 1)) inferInstance)
      (measurable_action _).comap_le (reward (n + 1)) (hist n) (ğ”“) :=
  condIndepFun_of_exists_condDistrib_prod_ae_eq_prodMkLeft
    (by fun_prop) (by fun_prop) (by fun_prop) (condDistrib_reward alg (stationaryEnv Î½) n)

end stationaryEnv

end Learning
